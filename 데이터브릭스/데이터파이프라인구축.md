```
기존에 Confluent로 구축한 pipeline을, Confluent license 이슈로,  재구축해야 합니다.
기존에서 변경되는 건은 sink 가 s3가 아니라, PostgreSQL입니다.

참고로 기존에는 Oracle Exadata  -> Confluent -> s3 -> Databricks -> PostgreSQL 로 구성되어 있었습니다.
참고로, to-be는 kafka가 아니어도 됩니다.
NiFi 등 다른 솔루션을 이용해도 됩니다.

```

```
01.Oracle Exadata 연동 드라이버 또는 커넥트 필요
02.워크플로우 프로그램 필요(NIFI, 에어플로우)
03.JOB 실행 엔진 필요
04.JOB 실행 로깅
05.장애 대응
06.ETL 작업 필요여부(데이터 가공 여부(전처리, 정제, 가공 등))
07.서버 자원 필요한데, AWS 서비스 활용여부 ???

```

![image](https://user-images.githubusercontent.com/102650331/188349624-5898ed58-4d76-417f-9dfc-fa606ea3a24e.png)

![image](https://user-images.githubusercontent.com/102650331/188349649-a0db20a9-94ec-4022-a1ec-c3a3f1e3b174.png)


# 검토 솔루션
- [Understanding Apache Airflow Streams Data Simplified 101] https://hevodata.com/learn/airflow-stream/
- https://www.youtube.com/watch?v=qFNzL957VO0
- [Event-driven architecture made easy with Apache NiFi] https://medium.com/quintoandar-tech-blog/event-driven-architecture-made-easy-with-apache-nifi-abb87c53f783
- https://medium.com/balancing-lines/automating-data-pipeline-using-apache-airflow-444e695181e9
- 
