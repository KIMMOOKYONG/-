# 패키지 설치
```shell
pip install nltk
pip install konlpy  
```

# 데이터 수집
# 전처리

## 영문
### 문장 토큰화
주어진 text를 sentence 단위로 tokenize함. 주로 . ! ? 등을 이용해서 문장 단위로 토크나이징함.
```python
from nltk.tokenize import sent_tokenize

para = "Hello everyone. It's good to see you. Let's start our text mining class!"
print(sent_tokenize(para))
```

### 단어 토큰화
```python
from nltk.tokenize import word_tokenize
from nltk.tokenize import WordPunctTokenizer

print(word_tokenize(para)) # 주어진 text를 word 단위로 tokenize함.
print(WordPunctTokenizer().tokenize(para)) # 구두점 "'" 및 word 단위로 tokenize함.
print(word_tokenize(para_kor))
```

### 정규표현식을 이용한 토큰화
```python
import re
re.findall("[abc]", "How are you, boy?")
re.findall("[0123456789]", "3a7b5c9d")
re.findall("[\w]", "3a 7b_ '.^&5c9d") # 영문자, 숫자, _
re.findall("[_]+", "a_b, c__d, e___f")
re.findall("[\w]+", "How are you, boy?") # 공백을 기준으로 단어 분리
re.findall("[o]{2,4}", "oh, hoow are yoooou, boooooooy?") # 문자 o가 2개에서 4개 추출

```

```python
from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer("[\w']+") # regular expression(정규식)을 이용한 tokenizer
#단어단위로 tokenize \w:문자나 숫자를 의미 즉 문자나 숫자 혹은 '가 반복되는 것을 찾아냄
print(tokenizer.tokenize("Sorry, I can't go there."))
# can't를 하나의 단어로 인식

text1 = "Sorry, I can't go there."
tokenizer = RegexpTokenizer("[\w']{3,}") 
print(tokenizer.tokenize(text1.lower()))

```

### 노이즈와 불용어 제거
```python
from nltk.corpus import stopwords # 일반적으로 분석대상이 아닌 단어들
from nltk.tokenize import RegexpTokenizer

english_stops = set(stopwords.words('english')) # 반복이 되지 않도록 set으로 변환
text1 = "Sorry, I couldn't go to movie yesterday."
tokenizer = RegexpTokenizer("[\w']+")
tokens = tokenizer.tokenize(text1.lower()) #word_tokenize로 토큰화
result = [word for word in tokens if word not in english_stops] # stopwords를 제외한 단어들만으로 list를 생성
print(result)

```

```python
from nltk.corpus import stopwords # 일반적으로 분석대상이 아닌 단어들
from nltk.tokenize import RegexpTokenizer

my_stopword = ["i", "go", "to"] # 나만의 stopword를 리스트로 정의
result = [word for word in tokens if word not in my_stopword] 
print(result)

```

## 한글
### 문장 토큰화
주어진 text를 sentence 단위로 tokenize함. 주로 . ! ? 등을 이용해서 문장 단위로 토크나이징함.
```python
from nltk.tokenize import sent_tokenize

para_kor = "안녕하세요, 여러분. 만나서 반갑습니다. 이제 텍스트마이닝 클래스를 시작해봅시다!"
print(sent_tokenize(para_kor))

```

### 단어 토큰화
```python
from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer("[\w']+") # regular expression(정규식)을 이용한 tokenizer
#단어단위로 tokenize \w:문자나 숫자를 의미 즉 문자나 숫자 혹은 '가 반복되는 것을 찾아냄
print(tokenizer.tokenize(para_kor))

```


# 정규화(Normalization)
## 영어
### 어간 추출(Stemming) - 단어의 어근 추출
```python
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()
print(stemmer.stem("cooking"), stemmer.stem("cookery"), stemmer.stem("cookbooks"))

from nltk.stem import LancasterStemmer
stemmer = LancasterStemmer()
print(stemmer.stem('cooking'), stemmer.stem('cookery'), stemmer.stem('cookbooks'))

```

```python
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
stemmer = PorterStemmer()

para = "Hello everyone. It's good to see you. Let's start our text mining class!"
tokens = word_tokenize(para) #토큰화 실행
print(tokens)
result = [stemmer.stem(token) for token in tokens] #모든 토큰에 대해 스테밍 실행
print(result)

```

### 표제어 추출(Lemmatization) - 품사 고려
표제어란 사전에서 단어의 뜻을 찾을 때 쓰는 기본형이라고 생각하면 된다.  

```python
# comparison of lemmatizing and stemming
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer

lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()
print("stemming result:", stemmer.stem("believes"))
print("lemmatizing result:", lemmatizer.lemmatize("believes"))
print("lemmatizing result:", lemmatizer.lemmatize("believes", pos="v"))

```

# 품사 태깅(Part-of-Speech Tagging)
## NLTK를 이용한 품사 태깅
```python
import nltk
from nltk.tokenize import word_tokenize

tokens = word_tokenize("Hello everyone. It's good to see you. Let's start our text mining class!")
my_tag_set = ['NN', 'VB', 'JJ']
my_words = [word for word, tag in nltk.pos_tag(tokens) if tag in my_tag_set]
print(my_words)

words_with_tag = ['/'.join(item) for item in nltk.pos_tag(tokens)]
print(words_with_tag)

```

## 한글 형태소 분석과 품사 태깅
```python
import nltk
from nltk.tokenize import word_tokenize

sentence = '''절망의 반대가 희망은 아니다.
어두운 밤하늘에 별이 빛나듯
희망은 절망 속에 싹트는 거지
만약에 우리가 희망함이 적다면
그 누가 세상을 비출어줄까.
정희성, 희망 공부'''

tokens = word_tokenize(sentence)
print(tokens)
print(nltk.pos_tag(tokens))

```

```python
from konlpy.tag import Okt
t = Okt()

print('형태소:', t.morphs(sentence))
print()
print('명사:', t.nouns(sentence))
print()
print('품사 태깅 결과:', t.pos(sentence))

```

# 카운트 기반의 문서표현
## BOW 기반의 카운트 벡터 생성
